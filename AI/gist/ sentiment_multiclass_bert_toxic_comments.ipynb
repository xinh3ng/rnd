{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" sentiment_multiclass_bert_toxic_comments.ipynb","provenance":[{"file_id":"1DLZSA_B9_tgwEeBMUW_B10EBi9tvnCFF","timestamp":1593824930085}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1bab588294064e45a1fad15f9a6ebaac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e3b0d46d7be34be7be13b4174a85f512","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_02c663f4bfe54b90aada20dfad8afe4a","IPY_MODEL_520991faa9314da8a440a0329856ad00"]}},"e3b0d46d7be34be7be13b4174a85f512":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"02c663f4bfe54b90aada20dfad8afe4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d508687107fe41e48d02f7059a6f8b2a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_231ea96b6bc5419c849b3ac12777e601"}},"520991faa9314da8a440a0329856ad00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0f3601a3c6c44630a5e5245e23cf5c38","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 257kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_773d0929c43d4093a6dfb68d9a6de71d"}},"d508687107fe41e48d02f7059a6f8b2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"231ea96b6bc5419c849b3ac12777e601":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f3601a3c6c44630a5e5245e23cf5c38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"773d0929c43d4093a6dfb68d9a6de71d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"KoN_QwOdGyWj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"ok","timestamp":1593897275148,"user_tz":420,"elapsed":5517,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"5127d429-49e5-4618-f121-158b099ba2ab"},"source":["\"\"\"\n","Links\n","  https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\n","  https://github.com/kaushaltrivedi/bert-toxic-comments-multilabel/blob/master/toxic-bert-multilabel-classification.ipynb\n","\n","\"\"\"\n","!pip install scikit-learn\n","!pip install transformers\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.15.1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jziiW-1VlXhB","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-XBS03BSRjof","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1593897510492,"user_tz":420,"elapsed":436,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"83243632-7bd2-4ef3-bac3-4db996c36d55"},"source":["# Libraries and general settings\n","import datetime\n","from google.colab import drive\n","import logging\n","import numpy as np\n","import os\n","import pandas as pd\n","from pathlib import Path\n","import random\n","from sklearn.metrics import (\n","    roc_curve, \n","    auc,\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    fbeta_score\n",")\n","from sklearn.model_selection import train_test_split\n","import sys\n","\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data import TensorDataset, random_split\n","from transformers import BertTokenizer, WordpieceTokenizer\n","from transformers import BertForPreTraining, BertPreTrainedModel, BertModel, BertConfig, BertForMaskedLM, BertForSequenceClassification\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","pd.set_option(\"precision\", 4)\n","\n","logging.basicConfig(\n","    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n","    datefmt='%m/%d/%Y %H:%M:%S',\n","    level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","module_path = os.path.abspath(os.path.join(\"..\"))\n","if module_path not in sys.path:\n","    sys.path.append(module_path)\n","\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print(\"%d GPU(s) available, namely %s\" % (torch.cuda.device_count(), torch.cuda.get_device_name(0)))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using CPU instead.')\n","\n","drive.mount(\"/content/gdrive\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 GPU(s) available, namely Tesla P100-PCIE-16GB\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-d0dkDb6FqMq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":223},"executionInfo":{"status":"error","timestamp":1593898421724,"user_tz":420,"elapsed":358,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"e813fc5e-5c43-47d9-bd3e-f06ef0e8580b"},"source":["def get_accuracy(labels, preds):\n","    \"\"\"Calculate the accuracy of our predictions vs labels\n","    \"\"\"\n","    labels_flat = labels.flatten()\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    accu = accuracy_score(labels_flat, pred_flat)\n","    return accu\n","\n","\n","def binary_clf_perf_scores(\n","    labels, preds, metrics: list = [\"accuracy\", \"f1\", \"precision\", \"recall\"], average: str = \"binary\"\n","):\n","    \"\"\"This is a convenience function. Performance scores that are suited for a binary classification problem.\n","    Args:\n","        y_true: True y values as a 1D vector\n","        y_pred: Predicted y values as 1D vector\n","        metrics:\n","        average: Averaging scheme. Default is \"binary\"\n","    Returns:\n","        One single Pandas row with selected metrics\n","    \"\"\"\n","    y_true = labels.flatten()\n","    y_pred = np.argmax(preds, axis=1).flatten()\n","\n","    perf = {}\n","    if \"accuracy\" in metrics:\n","        perf[\"accuracy\"] = accuracy_score(y_true, y_pred)\n","\n","    if \"precision\" in metrics:\n","        perf[\"precision\"] = precision_score(y_true, y_pred, average=average)\n","\n","    if \"recall\" in metrics:\n","        perf[\"recall\"] = recall_score(y_true, y_pred, average=average)\n","\n","    if \"f1\" in metrics:\n","        perf[\"f1\"] = fbeta_score(y_true, y_pred, beta=1, average=average)\n","        \n","    return perf\n","\n","\n","def accuracy(out, labels):\n","    outputs = np.argmax(out, axis=1)\n","    return np.sum(outputs == labels)\n","\n","\n","def accuracy_thresh(y_pred: Tensor, y_true : Tensor, thresh : float = 0.5, sigmoid : bool = True):\n","    \"\"\"Compute accuracy\n","    \"\"\"\n","    if sigmoid: \n","        y_pred = y_pred.sigmoid()\n","#       return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n","    return np.mean(((y_pred > thresh) == y_true.byte()).float().cpu().numpy(), axis=1).sum()\n","\n","\n","def fbeta(\n","    y_pred: Tensor, \n","    y_true: Tensor, \n","    thresh: float = 0.2, \n","    beta: float = 2, \n","    eps: float = 1e-9, \n","    sigmoid: bool = True):\n","    \"\"\"Computes the f_beta score\"\"\"\n","    beta2 = beta ** 2\n","    if sigmoid: \n","        y_pred = y_pred.sigmoid()\n","    \n","    y_pred = (y_pred > thresh).float()\n","    y_true = y_true.float()\n","    \n","    TP = (y_pred * y_true).sum(dim=1)\n","    prec = TP / (y_pred.sum(dim=1) + eps)\n","    rec = TP/(y_true.sum(dim=1)+eps) # recall\n","    res = (prec*rec) / (prec*beta2+rec+eps) * (1+beta2)\n","    return res.mean().item()\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-ba9147b0d44b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy_thresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \"\"\"Compute accuracy\n\u001b[1;32m     48\u001b[0m     \"\"\"\n","\u001b[0;31mNameError\u001b[0m: name 'Tensor' is not defined"]}]},{"cell_type":"code","metadata":{"id":"kBjfPmXEsJJ1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":361},"executionInfo":{"status":"error","timestamp":1593897515756,"user_tz":420,"elapsed":380,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"02f71978-0c6c-4030-956f-b0fa54854c00"},"source":["DATA_PATH = Path('../data/toxic_comments/')\n","DATA_PATH.mkdir(exist_ok=True)\n","\n","PATH = Path('../data/toxic_comments/tmp')\n","PATH.mkdir(exist_ok=True)\n","\n","CLAS_DATA_PATH = PATH/'class'\n","CLAS_DATA_PATH.mkdir(exist_ok=True)\n","\n","model_state_dict = None\n","\n","BERT_PRETRAINED_PATH = Path('../../complaints/bert/pretrained-weights/uncased_L-12_H-768_A-12/')\n","# BERT_PRETRAINED_PATH = Path('../../complaints/bert/pretrained-weights/uncased_L-24_H-1024_A-16/')\n","\n","# BERT_FINETUNED_WEIGHTS = Path('../trained_model/toxic_comments')\n","\n","PYTORCH_PRETRAINED_BERT_CACHE = BERT_PRETRAINED_PATH/'cache/'\n","PYTORCH_PRETRAINED_BERT_CACHE.mkdir(exist_ok=True)\n","\n","# output_model_file = os.path.join(BERT_FINETUNED_WEIGHTS, \"pytorch_model.bin\")\n","\n","# Load a trained model that you have fine-tuned\n","# model_state_dict = torch.load(output_model_file)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-00cacb63dffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/toxic_comments/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/toxic_comments/tmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mmkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(pathobj, *args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/toxic_comments'"]}]},{"cell_type":"code","metadata":{"id":"DXCilpktr6ca","colab_type":"code","colab":{}},"source":["\"\"\"Model Parameters\n","\"\"\"\n","\n","args = {\n","    \"train_size\": -1,\n","    \"val_size\": -1,\n","    \"full_data_dir\": DATA_PATH,\n","    \"data_dir\": PATH,\n","    \"task_name\": \"toxic_multilabel\",\n","    \"no_cuda\": False,\n","    \"bert_model\": BERT_PRETRAINED_PATH,\n","    \"output_dir\": CLAS_DATA_PATH/'output',\n","    \"max_seq_length\": 512,\n","    \"do_train\": True,\n","    \"do_eval\": True,\n","    \"do_lower_case\": True,\n","    \"train_batch_size\": 32,\n","    \"eval_batch_size\": 32,\n","    \"learning_rate\": 3e-5,\n","    \"num_train_epochs\": 4.0,\n","    \"warmup_proportion\": 0.1,\n","    \"no_cuda\": False,\n","    \"local_rank\": -1,\n","    \"seed\": 42,\n","    \"gradient_accumulation_steps\": 1,\n","    \"optimize_on_cpu\": False,\n","    \"fp16\": False,\n","    \"loss_scale\": 128\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bbalx_Ju5wh","colab_type":"code","colab":{}},"source":["class DataProcessor(object):\n","    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        raise NotImplementedError()\n","    \n","    def get_test_examples(self, data_dir, data_file_name, size=-1):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        raise NotImplementedError() \n","\n","    def get_labels(self):\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\n","        raise NotImplementedError()\n","\n","\n","class MultiLabelTextProcessor(DataProcessor):\n","    \n","    def __init__(self, data_dir):\n","        self.data_dir = data_dir\n","        self.labels = None\n","    \n","    \n","    def get_train_examples(self, data_dir, size=-1):\n","        filename = 'train.csv'\n","        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, filename)))\n","        if size == -1:\n","            data_df = pd.read_csv(os.path.join(data_dir, filename))\n","#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n","            return self._create_examples(data_df, \"train\")\n","        else:\n","            data_df = pd.read_csv(os.path.join(data_dir, filename))\n","#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n","            return self._create_examples(data_df.sample(size), \"train\")\n","        \n","    def get_dev_examples(self, data_dir, size=-1):\n","        \"\"\"See base class.\"\"\"\n","        filename = 'val.csv'\n","        if size == -1:\n","            data_df = pd.read_csv(os.path.join(data_dir, filename))\n","#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n","            return self._create_examples(data_df, \"dev\")\n","        else:\n","            data_df = pd.read_csv(os.path.join(data_dir, filename))\n","#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n","            return self._create_examples(data_df.sample(size), \"dev\")\n","    \n","    def get_test_examples(self, data_dir, data_file_name, size=-1):\n","        data_df = pd.read_csv(os.path.join(data_dir, data_file_name))\n","#         data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n","        if size == -1:\n","            return self._create_examples(data_df, \"test\")\n","        else:\n","            return self._create_examples(data_df.sample(size), \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        if self.labels == None:\n","            self.labels = list(pd.read_csv(os.path.join(self.data_dir, \"classes.txt\"),header=None)[0].values)\n","        return self.labels\n","\n","    def _create_examples(self, df, set_type, labels_available=True):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        examples = []\n","        for (i, row) in enumerate(df.values):\n","            guid = row[0]\n","            text_a = row[1]\n","            if labels_available:\n","                labels = row[2:]\n","            else:\n","                labels = []\n","            examples.append(\n","                InputExample(guid=guid, text_a=text_a, labels=labels))\n","        return examples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ONJNFlXu_xG","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FeONOOdIHiIJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":398,"referenced_widgets":["1bab588294064e45a1fad15f9a6ebaac","e3b0d46d7be34be7be13b4174a85f512","02c663f4bfe54b90aada20dfad8afe4a","520991faa9314da8a440a0329856ad00","d508687107fe41e48d02f7059a6f8b2a","231ea96b6bc5419c849b3ac12777e601","0f3601a3c6c44630a5e5245e23cf5c38","773d0929c43d4093a6dfb68d9a6de71d"]},"executionInfo":{"status":"ok","timestamp":1593825039914,"user_tz":420,"elapsed":8132,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"f730ea9e-7ae0-43ad-8050-2753a36e3c09"},"source":["tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)  # \n","\n","rawdata = pd.read_csv(\n","    \"/content/gdrive/My Drive/data team/AI/projects/sensus/data/labels/deepsentiment - annotated customer reviews - punchh - 2018-master.tsv\",\n","    delimiter=\"\\t\"\n","    )\n","print(\"raw data has %d rows\" %(len(rawdata)))\n","\n","rawdata.sample(10)\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1bab588294064e45a1fad15f9a6ebaac","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","raw data has 2700 rows\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>message</th>\n","      <th>guest_rating</th>\n","      <th>feedback_id</th>\n","      <th>business_id</th>\n","      <th>created_at</th>\n","      <th>customer_service</th>\n","      <th>food_quality</th>\n","      <th>app_program</th>\n","      <th>ambience</th>\n","      <th>waiting_time</th>\n","      <th>polarity</th>\n","      <th>conflict</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1388</th>\n","      <td>2018-08</td>\n","      <td>great customer service</td>\n","      <td>NaN</td>\n","      <td>8610932.0</td>\n","      <td>628.0</td>\n","      <td>2018-08-11 00:18:13.000</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>2018-09</td>\n","      <td>My meal is always delicious, but I absolutely ...</td>\n","      <td>5.0</td>\n","      <td>9125144.0</td>\n","      <td>565.0</td>\n","      <td>2018-09-08 22:53:03.000</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>138</th>\n","      <td>2018-09</td>\n","      <td>I was really sad to hear that you discontinued...</td>\n","      <td>2.0</td>\n","      <td>9119745.0</td>\n","      <td>628.0</td>\n","      <td>2018-09-08 18:57:34.000</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>2018-09</td>\n","      <td>I love you guys</td>\n","      <td>NaN</td>\n","      <td>9069345.0</td>\n","      <td>561.0</td>\n","      <td>2018-09-05 22:09:22.000</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>692</th>\n","      <td>2018-09</td>\n","      <td>1st set of 100 points. Awesome and easy to sca...</td>\n","      <td>NaN</td>\n","      <td>9598458.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1240</th>\n","      <td>2018-09</td>\n","      <td>I love the food and order was late。</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1808</th>\n","      <td>2018-07</td>\n","      <td>good food and fast</td>\n","      <td>5.0</td>\n","      <td>7946119.0</td>\n","      <td>642.0</td>\n","      <td>2018-07-01 19:23:48.000</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>346</th>\n","      <td>2018-09</td>\n","      <td>Love love love!!!</td>\n","      <td>5.0</td>\n","      <td>9326456.0</td>\n","      <td>634.0</td>\n","      <td>2018-09-19 20:49:24.000</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1447</th>\n","      <td>2018-08</td>\n","      <td>So yummy!!</td>\n","      <td>NaN</td>\n","      <td>8527481.0</td>\n","      <td>638.0</td>\n","      <td>2018-08-06 17:04:50.000</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>2018-09</td>\n","      <td>I always love eating at pieology!</td>\n","      <td>5.0</td>\n","      <td>9009397.0</td>\n","      <td>631.0</td>\n","      <td>2018-09-02 08:45:43.000</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       source  ... conflict\n","1388  2018-08  ...      0.0\n","145   2018-09  ...      0.0\n","138   2018-09  ...      0.0\n","97    2018-09  ...      0.0\n","692   2018-09  ...      0.0\n","1240  2018-09  ...      1.0\n","1808  2018-07  ...      0.0\n","346   2018-09  ...      0.0\n","1447  2018-08  ...      0.0\n","32    2018-09  ...      0.0\n","\n","[10 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"qY863p4NuEZD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":400},"executionInfo":{"status":"ok","timestamp":1593755495338,"user_tz":420,"elapsed":673,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"8cac0d42-c042-4edb-c1a4-76c890e925d8"},"source":["# Get the lists of messages and their labels.\n","metric = \"polarity\"\n","data = rawdata.loc[pd.notna(rawdata[metric])].copy(deep=True)\n","data = data[[\"message\", \"food_quality\", \"polarity\"]]\n","\n","print(\"Unique values: %s\" %(set(data[metric])))\n","print(\"data has %d rows\" %(len(data)))\n","print(\"data has %d one's and %d zero's\" %(sum(data[metric] == 1),sum(data[metric] == 0)))\n","\n","sentences = data[\"message\"].values\n","labels = data[metric].astype(int).values\n","\n","# Get the longest sentence\n","max_len = 0\n","for sent in sentences:\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","    max_len = max(max_len, len(input_ids))\n","print(\"Max sentence length: \", max_len)\n","\n","data.sample(10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unique values: {0.0, 1.0}\n","data has 135 rows\n","data has 61 one's and 74 zero's\n","Max sentence length:  325\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>message</th>\n","      <th>guest_rating</th>\n","      <th>feedback_id</th>\n","      <th>business_id</th>\n","      <th>created_at</th>\n","      <th>customer_service</th>\n","      <th>food_quality</th>\n","      <th>app_program</th>\n","      <th>ambience</th>\n","      <th>waiting_time</th>\n","      <th>polarity</th>\n","      <th>conflict</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1483</th>\n","      <td>2018-08</td>\n","      <td>Service is fast and food is excellent!</td>\n","      <td>5.0</td>\n","      <td>8.5658e+06</td>\n","      <td>628.0</td>\n","      <td>2018-08-08 17:41:05.000</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>301</th>\n","      <td>2018-09</td>\n","      <td>I am always surprised by how poor the service ...</td>\n","      <td>2.0</td>\n","      <td>9.2873e+06</td>\n","      <td>561.0</td>\n","      <td>2018-09-17 21:10:28.000</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1948</th>\n","      <td>2018-07</td>\n","      <td>Good food and a bit too long of a wait.</td>\n","      <td>4.0</td>\n","      <td>8.0985e+06</td>\n","      <td>628.0</td>\n","      <td>2018-07-11 19:29:24.000</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1968</th>\n","      <td>2018-07</td>\n","      <td>Customer Service is seriously lacking. Had to ...</td>\n","      <td>1.0</td>\n","      <td>8.1160e+06</td>\n","      <td>622.0</td>\n","      <td>2018-07-12 21:10:23.000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1240</th>\n","      <td>2018-09</td>\n","      <td>I love the food and order was late。</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1821</th>\n","      <td>2018-07</td>\n","      <td>Great staff. No wait!</td>\n","      <td>5.0</td>\n","      <td>7.9535e+06</td>\n","      <td>628.0</td>\n","      <td>2018-07-02 02:21:00.000</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2574</th>\n","      <td>2018-06</td>\n","      <td>Quick and tastefully.</td>\n","      <td>5.0</td>\n","      <td>7.8081e+06</td>\n","      <td>611.0</td>\n","      <td>2018-06-22 17:44:51.000</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>868</th>\n","      <td>2018-09</td>\n","      <td>Chloe is awesome.  I wish I could get my team ...</td>\n","      <td>NaN</td>\n","      <td>9.8983e+06</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2314</th>\n","      <td>2018-06</td>\n","      <td>Very slow service but the food was good!</td>\n","      <td>NaN</td>\n","      <td>7.5500e+06</td>\n","      <td>637.0</td>\n","      <td>2018-06-05 15:32:25.000</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>877</th>\n","      <td>2018-09</td>\n","      <td>They left us waiting for a good 45min. Forgett...</td>\n","      <td>NaN</td>\n","      <td>1.0005e+07</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       source  ... conflict\n","1483  2018-08  ...      0.0\n","301   2018-09  ...      0.0\n","1948  2018-07  ...      1.0\n","1968  2018-07  ...      0.0\n","1240  2018-09  ...      1.0\n","1821  2018-07  ...      0.0\n","2574  2018-06  ...      0.0\n","868   2018-09  ...      1.0\n","2314  2018-06  ...      0.0\n","877   2018-09  ...      1.0\n","\n","[10 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"Jh0BvvtTujJi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":655},"executionInfo":{"status":"ok","timestamp":1593755496523,"user_tz":420,"elapsed":625,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"5f9ff83f-1d24-42dd-a4b6-5bbff0669146"},"source":["\"\"\"\n","Tokenize all of the sentences\n","\"\"\"\n","max_length = 128\n","\n","# Tokenize all of the sentences and map the tokens to thier word ids \n","input_ids = []\n","attention_masks = []\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","        sent,\n","        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n","        max_length = max_length,  # Pad & truncate all sentences\n","        truncation=True,\n","        pad_to_max_length = True,\n","        return_attention_mask = True,\n","        return_tensors = \"pt\",  # Return pytorch tensors\n","        )\n","    input_ids.append(encoded_dict[\"input_ids\"])  # Add the encoded sentence to the list.   \n","    attention_masks.append(encoded_dict[\"attention_mask\"])  # And attn mask (differentiates padding from non-padding) \n","\n","# Convert the lists into tensors\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# Print an example sentence as an example\n","for id in [0, 10, 100]:\n","    print(\"Original: \", sentences[id])\n","    print(\"Token IDs:\", input_ids[id])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  The service was fast and very friendly\n","Token IDs: tensor([ 101, 1996, 2326, 2001, 3435, 1998, 2200, 5379,  102,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n","Original:  Quick, friendly and easy to order\n","Token IDs: tensor([ 101, 4248, 1010, 5379, 1998, 3733, 2000, 2344,  102,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n","Original:  Customer Service is seriously lacking. Had to ask for a receipt, the cashier just walked away after taking order/money. Also, after the order was ready. I said thank you, and the same person that had taken the order and made our drinks. She just said yup after I said thank you. Even though we order the same drink every time, it never tastes the same. Guess we should stick with Starbucks.\n","Token IDs: tensor([  101,  8013,  2326,  2003,  5667, 11158,  1012,  2018,  2000,  3198,\n","         2005,  1037, 24306,  1010,  1996,  5356,  3771,  2074,  2939,  2185,\n","         2044,  2635,  2344,  1013,  2769,  1012,  2036,  1010,  2044,  1996,\n","         2344,  2001,  3201,  1012,  1045,  2056,  4067,  2017,  1010,  1998,\n","         1996,  2168,  2711,  2008,  2018,  2579,  1996,  2344,  1998,  2081,\n","         2256,  8974,  1012,  2016,  2074,  2056,  9805,  2361,  2044,  1045,\n","         2056,  4067,  2017,  1012,  2130,  2295,  2057,  2344,  1996,  2168,\n","         4392,  2296,  2051,  1010,  2009,  2196, 16958,  1996,  2168,  1012,\n","         3984,  2057,  2323,  6293,  2007, 29500,  1012,   102,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9DIq6ezGu31-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1593755497914,"user_tz":420,"elapsed":836,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"0474d902-1918-4e08-b1f0-c29130730488"},"source":["# Create Train and test sets\n","dataset = TensorDataset(input_ids, attention_masks, labels) \n","train_size = int(0.9 * len(dataset)) # 90-10 train-validation split\n","val_size = len(dataset) - train_size\n","train_set, val_set = random_split(dataset, [train_size, val_size])\n","\n","print('There are {:>5,} training samples'.format(train_size))\n","print('There are {:>5,} validation samples'.format(val_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are   121 training samples\n","There are    14 validation samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M-A4qJIQvLBB","colab_type":"code","colab":{}},"source":["# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoaders for our train and val sets\n","train_dataloader = DataLoader(\n","    train_set,  # The training samples.\n","    sampler=RandomSampler(train_set),  # select batches randomly\n","    batch_size=batch_size # Trains with this batch size.\n","    )\n","\n","validation_dataloader = DataLoader(\n","    val_set, # The validation samples.\n","    sampler=SequentialSampler(val_set), # Pull sequentially.\n","    batch_size=batch_size  # Evaluate with this batch size.\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSw-Xv_RvXPJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593755507025,"user_tz":420,"elapsed":7068,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"6ccb618b-36e6-4586-d609-baa0610e6c2a"},"source":["# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\",\n","    num_labels=2, # 2 for binary classification.\n","    output_attentions=False, # Whether to returns attention weights.\n","    output_hidden_states = False, # Whether to return all hidden states.\n",")\n","model.cuda()  # Tell PyTorch to run this model on the GPU \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"TJgyWvnfve67","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":586},"executionInfo":{"status":"ok","timestamp":1593755507026,"user_tz":420,"elapsed":331,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"65fb6cb8-5497-457e-d48a-30ed0825c471"},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","print('==== Embedding Layer ====\\n')\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","print('\\n==== Output Layer ====\\n')\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The BERT model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                  (30522, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dliTZSC2v1r6","colab_type":"code","colab":{}},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr=2e-5,  # args.learning_rate, default is 5e-5, our notebook had 2e-5\n","                  eps=1e-8  # args.adam_epsilon, default is 1e-8.\n","                )\n","# The BERT authors recommend between 2 and 4\n","epochs = 4\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, \n","    num_warmup_steps = 0, # Default value in run_glue.py\n","    num_training_steps = total_steps)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6_D8ZRAwxln","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":781},"executionInfo":{"status":"ok","timestamp":1593755530150,"user_tz":420,"elapsed":21438,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"8ba30296-f1a2-41a8-91aa-baa46cbb567f"},"source":["# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 40  # 42 is the original value \n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        loss, logits = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy, total_eval_precision, total_eval_recall, total_eval_f1_score = 0, 0, 0, 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            (loss, logits) = model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels)\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        scores = binary_clf_perf_scores(label_ids, logits)\n","\n","        total_eval_accuracy += scores[\"accuracy\"]\n","        total_eval_precision += scores[\"precision\"]\n","        total_eval_recall += scores[\"recall\"]\n","        total_eval_f1_score += scores[\"f1\"]\n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    avg_val_precision = total_eval_precision / len(validation_dataloader)\n","    avg_val_recall = total_eval_recall / len(validation_dataloader)\n","    avg_val_f1_score = total_eval_f1_score / len(validation_dataloader)\n","    \n","    avg_val_loss = total_eval_loss / len(validation_dataloader)    \n","    validation_time = format_time(time.time() - t0)\n","\n","    print(\"  Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    training_stats.append(\n","        {\n","            \"epoch\": epoch_i + 1,\n","            'train_loss': avg_train_loss,\n","            'val_loss': avg_val_loss,\n","            'val_accuracy': avg_val_accuracy,\n","            \"val_precision\": avg_val_precision,\n","            \"val_recall\": avg_val_recall,\n","            \"val_f1_score\": avg_val_f1_score\n","        }\n","    )\n","print(\"\\nSuccessfully finished training\")\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.74\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Validation Accuracy: 0.3571\n","  Validation Loss: 0.75\n","  Validation took: 0:00:00\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.55\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Validation Accuracy: 0.6429\n","  Validation Loss: 0.71\n","  Validation took: 0:00:00\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.50\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Validation Accuracy: 0.6429\n","  Validation Loss: 0.67\n","  Validation took: 0:00:00\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.41\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Validation Accuracy: 0.7143\n","  Validation Loss: 0.66\n","  Validation took: 0:00:00\n","\n","Successfully finished training\n","Total training took 0:00:21 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DR1rt8fIw2zf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1593755530151,"user_tz":420,"elapsed":15735,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"}},"outputId":"e81058be-4d2f-4a0f-a6c0-15a5f8aec902"},"source":["stats = pd.DataFrame(data=training_stats).set_index(\"epoch\")\n","stats"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_loss</th>\n","      <th>val_loss</th>\n","      <th>val_accuracy</th>\n","      <th>val_precision</th>\n","      <th>val_recall</th>\n","      <th>val_f1_score</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.7362</td>\n","      <td>0.7501</td>\n","      <td>0.3571</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.5477</td>\n","      <td>0.7130</td>\n","      <td>0.6429</td>\n","      <td>0.5</td>\n","      <td>0.6</td>\n","      <td>0.5455</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.4978</td>\n","      <td>0.6717</td>\n","      <td>0.6429</td>\n","      <td>0.5</td>\n","      <td>0.6</td>\n","      <td>0.5455</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.4102</td>\n","      <td>0.6587</td>\n","      <td>0.7143</td>\n","      <td>0.6</td>\n","      <td>0.6</td>\n","      <td>0.6000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       train_loss  val_loss  ...  val_recall  val_f1_score\n","epoch                        ...                          \n","1          0.7362    0.7501  ...         0.0        0.0000\n","2          0.5477    0.7130  ...         0.6        0.5455\n","3          0.4978    0.6717  ...         0.6        0.5455\n","4          0.4102    0.6587  ...         0.6        0.6000\n","\n","[4 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"ZIaG9dPQyBPV","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}