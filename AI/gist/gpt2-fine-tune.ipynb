{"cells":[{"cell_type":"markdown","metadata":{"id":"1JWjZskRT1mr"},"source":["**Links:**\n","\n","*   https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912\n","*   https://gist.github.com/mf1024/3df214d2f17f3dcc56450ddf0d5a4cd7#file-fine-tuning-gpt2-medium-in-pytorch-ipynb\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ih_TLLX0T1ms"},"source":["# Generating text with a pre-trained GPT2 in PyTorch\n","\n","This notebook was created as a part of a blog post - [Fine-tuning large Transformer models on a single GPU in PyTorch - Teaching GPT-2 a sense of humor](https://mf1024.github.io/2019/11/12/Fun-With-GPT-2/).\n","\n","In this notebook, I will use a pre-trained medium-sized GPT2 model from the [huggingface](https://github.com/huggingface/transformers) to generate some text.\n","\n","The easiest way to use huggingface transformer libraries is to install their pip package *transformers*."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":570},"executionInfo":{"elapsed":6830,"status":"ok","timestamp":1602815580064,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"},"user_tz":420},"id":"JVi53PP4T1mt","outputId":"2bd4a7a2-5da0-4f1f-abff-24b3d641ed6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n","\r\u001b[K     |▎                               | 10kB 23.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 7.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 9.4MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 9.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 9.2MB/s eta 0:00:01\r\u001b[K     |███▊                            | 122kB 9.2MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 9.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 9.2MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 174kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 184kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 204kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 215kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 225kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 245kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 256kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 276kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 286kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 296kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 307kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 317kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 327kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 337kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 348kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 358kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 368kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 378kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 389kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 409kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 419kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 430kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 440kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 450kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 460kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 471kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 481kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 491kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 501kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 512kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 522kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 532kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 542kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 552kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 563kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 573kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 583kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 593kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 604kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 614kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 624kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 634kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 645kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 655kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 665kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 675kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 686kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 696kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 706kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 716kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 727kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 737kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 747kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 757kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 768kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 778kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 788kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 798kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 808kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 819kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 829kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 839kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 849kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 860kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 870kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 880kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 890kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 901kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 911kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 921kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 931kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 942kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 952kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 962kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 972kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 983kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 993kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.0MB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.0MB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1MB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 9.2MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting tokenizers==0.8.1.rc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 32.7MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 50.2MB/s \n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version \u003c \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 51.0MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: pyparsing\u003e=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-\u003etransformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging-\u003etransformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-\u003etransformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-\u003etransformers) (0.16.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers) (2020.6.20)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=1ece2dd25263c78204778821aff6f47fe9a217b7378ffc45f0fd4b6b07277b14\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":82},"executionInfo":{"elapsed":33019,"status":"ok","timestamp":1602815606260,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"},"user_tz":420},"id":"7KrlR4r4T1mw","outputId":"db1bd765-ef2b-4d45-8857-07748af87926"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python version is 3.6.9 (default, Jul 17 2020, 12:50:27) \n","[GCC 8.4.0]\n","Device is: cuda\n","Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","import logging\n","import numpy as np\n","import pandas as pd\n","import sys\n","import torch\n","\n","logging.getLogger().setLevel(logging.CRITICAL)\n","\n","device = \"cpu\"\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","\n","pd.set_option(\"precision\", 4)\n","\n","print(\"Python version is %s\" % sys.version)\n","print(\"Device is: %s\" % device)\n","\n","drive.mount(\"/content/gdrive\")\n"]},{"cell_type":"markdown","metadata":{"id":"EScszWmxT1mz"},"source":["### Models and classes\n","\n","I use the [GPT2LMHeadModel](https://github.com/huggingface/transformers/blob/master/transformers/modeling_gpt2.py#L491) module for the language model, which is [GPT2Model](https://github.com/huggingface/transformers/blob/master/transformers/modeling_gpt2.py#L326), with an additional linear layer that uses input embedding layer weights to do the inverse operation of the embedding layer - to create logits vector for the dictionary from outputs of the GPT2.\n","\n","[GPT2Tokenizer](https://github.com/huggingface/transformers/blob/master/transformers/tokenization_gpt2.py#L106) is a byte-code pair encoder that will transform input text input into input tokens that the huggingface transformers were trained on. "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"executionInfo":{"elapsed":81466,"status":"ok","timestamp":1602815654714,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"},"user_tz":420},"id":"zUX7jIfoT1m0","outputId":"5bcbe777-5d75-443f-8bbc-7754c70675bb"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2a94126efe7456ca541b877625c788d","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf9056f6b35441b68bc983441937c9d4","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d91a6b0f40064a0ab6944f5a5bf71c18","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d1a43d9541740d5a6641e1d1bc68a9a","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","model has 56 Bytes\n"]}],"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","model = model.to(device)\n","print(\"model has %s Bytes\" % sys.getsizeof(model))"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":81461,"status":"ok","timestamp":1602815654715,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"},"user_tz":420},"id":"uUkseDkIT1m3"},"outputs":[],"source":["def choose_from_top(probs: list, n: int = 5):\n","    \"\"\"Select topN tokens from the probability list. Then based on the selected N word distribution get random token ID\n","    \"\"\"\n","    ind = np.argpartition(probs, -n)[-n:]\n","    top_prob = probs[ind]\n","    top_prob = top_prob / np.sum(top_prob) # Normalize\n","    choice = np.random.choice(n, 1, p = top_prob)\n","    token_id = ind[choice][0]\n","    return int(token_id)"]},{"cell_type":"markdown","metadata":{"id":"cWCSFbfmT1m5"},"source":["### Text generation\n","\n","At each prediction step, GPT2 model needs to know all of the previous sequence elements to predict the next one. Below is a function that will tokenize the starting input text, and then in a loop, one new token is predicted at each step and is added to the sequence, which will be fed into the model in the next step. In the end, the token list is decoded back into a text. "]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":81457,"status":"ok","timestamp":1602815654715,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"},"user_tz":420},"id":"KiZtB09ET1m5"},"outputs":[],"source":["def generate_some_text(model, input_str: str, text_len: int = 250):\n","    cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for i in range(text_len):\n","            outputs = model(cur_ids, labels=cur_ids)\n","            loss, logits = outputs[:2]\n","\n","            # Take the first(only one) batch and the last predicted embedding\n","            softmax_logits = torch.softmax(logits[0,-1], dim=0) \n","            \n","            # Randomly(from the given probability distribution) choose the next word from the top n words\n","            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=10) \n","            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word\n","        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n","        output_text = tokenizer.decode(output_list)\n","        print(output_text)\n","    return    "]},{"cell_type":"markdown","metadata":{"id":"FxZE8sBUT1m8"},"source":["## Generating the text\n","\n","I will give thre different sentence beginnings to the GPT2 and let it generate the rest:\n","\n","\n","***1. The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work… when you go to church… when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth…***\n","\n","***2. Artificial general intelligence is…***\n","\n","***3. The Godfather: “I’m going to make him an offer he can’t refuse.”…***"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"elapsed":87082,"status":"ok","timestamp":1602815660345,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"},"user_tz":420},"id":"Ec1tIq_tT1m8","outputId":"f3e8f579-6f7c-4009-c300-f30dde7c47ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work... when you go to church... when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth.  We are all connected.\n","The Matrix is here now, and we are not alone. In fact many of you know that the Matrix and the Matrix are the same. We are all part of a network of interconnected entities. The Matrix is a part of this world, a part of the fabric of existence. It is an interconnected entity, and that makes it a part of our universe. The Matrix is our reality, but it is not your reality. We are part of a network of entities, that makes us all part of each other.\n","This is the Matrix and it is our reality. We are all part of the Matrix. We are part of our universe, and that makes it part of our universe. We exist in our own universe, that is part our universe. We live in a living universe, and that makes this whole world of our being, our reality, part of our reality.\n","The Matrix is an illusion, an illusion of our reality, part of our reality. We exist in a living universe, and that makes this whole of our being, this whole being, part of our reality.\n","You are in the Matrix. You see it when you go to work, when you go to church... when you\n"]}],"source":["generate_some_text(model, \"The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work... when you go to church... when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth. \")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"elapsed":91368,"status":"ok","timestamp":1602815664637,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"},"user_tz":420},"id":"XLphHAyNT1nA","outputId":"60b2ab41-e04b-49ec-95bd-601beae2a328"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Artificial general intelligence is  not  something you can predict by looking at your brain. You can't predict anything, you can only predict the probability of something happening. This means that if a machine were to learn to predict a certain outcome in a certain way, it'd probably be a lot more likely to be able to predict that outcome. If the machine were to learn to predict a certain outcome in a certain way, then you'd be able to predict it, because the outcome you predict in the future doesn't exist in the past.\n","The next big thing is that machine learning is just not going to be very efficient when it's not working out that it's a good strategy.\n","This is why it's important to remember that you're not going to have much fun with it. It's not the best strategy to be a robot because it will always be more effective and it will probably never be that effective when it's just learning. If the machine were to learn how to pick up objects and find them in a specific way, it would probably be an extremely good strategy. But it would be a much better strategy than it is right now. The machine will learn to learn in a way that will make it more efficient, and it will learn more\n"]}],"source":["generate_some_text(model, \"Artificial general intelligence is \")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"elapsed":95988,"status":"ok","timestamp":1602815669263,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"},"user_tz":420},"id":"z1i7X_39T1nD","outputId":"376ad4b4-a9e1-4e11-94ee-e0c7030367bb"},"outputs":[{"name":"stdout","output_type":"stream","text":[" The Godfather: \"I'm going to make him an offer he can't refuse.\"  It's a great deal. I'm looking forward to making it to the next level. But I'm not sure that the story will end up in the same place. \n","A couple of weeks ago, I received an email from the writer of The Godfather  on the subject of \"Godfather: A Christmas Story\".  \"I'm going to give the movie a big ol' Christmas present,\" wrote the writer. He's a big ol' Christian, and he doesn't have an official Christmas story, so I'm not sure what he's talking about.  So I went to the movie's official website and found an email from a writer for the film's director. It's called \"I'm going to make you a Christmas wish.\"  This is a very specific request.  I'm asking for an \"official\" Christmas story.  I'm also asking for one that doesn't have an official \"Godfather: A Christmas Story\".  So, if you read this, you'll be aware that The Godfather: A Christmas Story has no official \"Christmas story\" story.  And there's no official Christmas story about it.\n","The Godfather: A Christmas Story\n"]}],"source":["generate_some_text(model, \"The Godfather: \\\"I'm going to make him an offer he can't refuse.\\\" \")"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":98755,"status":"ok","timestamp":1602815672036,"user":{"displayName":"Xin Heng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghih2qYah2j147oGatNY6HQBB8nFM3Wl0KUFG7X=s64","userId":"10806682243950504635"},"user_tz":420},"id":"dBmcRsiIVgGy"},"outputs":[],"source":["\"\"\"\n","Jokes data set\n","\"\"\"\n","import csv\n","import os\n","import json\n","\n","from torch.utils.data import Dataset\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","class JokesDataset(Dataset):\n","    def __init__(self, jokes_dataset_path: str):\n","        super().__init__()\n","        short_jokes_path = os.path.join(jokes_dataset_path, \"shortjokes.csv\")\n","        self.joke_list = []\n","        self.end_of_text_token = \"\u003c|endoftext|\u003e\"\n","        \n","        with open(short_jokes_path) as csv_file:\n","            csv_reader = csv.reader(csv_file, delimiter=',')\n","            x = 0\n","            for row in csv_reader:\n","                joke_str = f\"JOKE:{row[1]}{self.end_of_text_token}\"\n","                self.joke_list.append(joke_str)\n","        \n","    def __len__(self):\n","        return len(self.joke_list)\n","\n","    def __getitem__(self, item):\n","        return self.joke_list[item]\n","\n","jokes_dataset_path = \"/content/gdrive/My Drive/xheng/data/jokes_data/\"  # flower dataset's path\n","\n","dataset = JokesDataset(jokes_dataset_path=jokes_dataset_path)\n","joke_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"k_aFvphHV94i","outputId":"489e78f3-121f-4ee3-f507-07c5fae407f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH: 0 started\n","batch_count = 10, sum_loss = 734.0015258789062\n","batch_count = 10, sum_loss = 730.8923950195312\n","batch_count = 10, sum_loss = 734.0779418945312\n","batch_count = 10, sum_loss = 726.9974975585938\n","batch_count = 10, sum_loss = 724.5185546875\n","batch_count = 10, sum_loss = 726.5108032226562\n","batch_count = 10, sum_loss = 718.717529296875\n","batch_count = 10, sum_loss = 720.8909912109375\n","batch_count = 10, sum_loss = 716.4512939453125\n","batch_count = 10, sum_loss = 709.8035278320312\n","batch_count = 10, sum_loss = 704.7301025390625\n","batch_count = 10, sum_loss = 698.413330078125\n","batch_count = 10, sum_loss = 694.8424682617188\n","batch_count = 10, sum_loss = 690.5439453125\n","batch_count = 10, sum_loss = 682.8909301757812\n","batch_count = 10, sum_loss = 675.9745483398438\n","batch_count = 10, sum_loss = 669.7047119140625\n","batch_count = 10, sum_loss = 665.923583984375\n","batch_count = 10, sum_loss = 657.6903076171875\n","batch_count = 10, sum_loss = 653.5399780273438\n","batch_count = 10, sum_loss = 642.8981323242188\n","batch_count = 10, sum_loss = 635.2879028320312\n","batch_count = 10, sum_loss = 625.1593627929688\n","batch_count = 10, sum_loss = 616.8165283203125\n","batch_count = 10, sum_loss = 620.570556640625\n","batch_count = 10, sum_loss = 611.005615234375\n","batch_count = 10, sum_loss = 614.4714965820312\n","batch_count = 10, sum_loss = 611.5473022460938\n","batch_count = 10, sum_loss = 609.7984619140625\n","batch_count = 10, sum_loss = 604.5213012695312\n","batch_count = 10, sum_loss = 602.0669555664062\n","batch_count = 10, sum_loss = 604.8258666992188\n","batch_count = 10, sum_loss = 602.2195434570312\n","batch_count = 10, sum_loss = 603.428955078125\n","batch_count = 10, sum_loss = 594.5695190429688\n","batch_count = 10, sum_loss = 598.604248046875\n","batch_count = 10, sum_loss = 593.2224731445312\n","batch_count = 10, sum_loss = 596.1047973632812\n","batch_count = 10, sum_loss = 593.3184814453125\n","batch_count = 10, sum_loss = 591.5445556640625\n","batch_count = 10, sum_loss = 592.0277099609375\n","batch_count = 10, sum_loss = 588.5961303710938\n","batch_count = 10, sum_loss = 591.1624755859375\n","batch_count = 10, sum_loss = 590.6891479492188\n","batch_count = 10, sum_loss = 593.1396484375\n","batch_count = 10, sum_loss = 591.12744140625\n","batch_count = 10, sum_loss = 589.2972412109375\n","batch_count = 10, sum_loss = 585.9771118164062\n","batch_count = 10, sum_loss = 587.1552124023438\n","batch_count = 10, sum_loss = 584.5311889648438\n","batch_count = 10, sum_loss = 582.9190063476562\n","batch_count = 10, sum_loss = 586.4868774414062\n","batch_count = 10, sum_loss = 585.4404296875\n","batch_count = 10, sum_loss = 580.9052734375\n","batch_count = 10, sum_loss = 578.8553466796875\n","batch_count = 10, sum_loss = 581.2015380859375\n","batch_count = 10, sum_loss = 576.4989624023438\n","batch_count = 10, sum_loss = 583.3291015625\n","batch_count = 10, sum_loss = 579.1950073242188\n","batch_count = 10, sum_loss = 580.8191528320312\n","batch_count = 10, sum_loss = 580.3926391601562\n","batch_count = 10, sum_loss = 578.3876342773438\n","batch_count = 10, sum_loss = 574.5697631835938\n","batch_count = 10, sum_loss = 576.5841674804688\n","batch_count = 10, sum_loss = 575.7891235351562\n","batch_count = 10, sum_loss = 574.6917114257812\n","batch_count = 10, sum_loss = 573.6326904296875\n","batch_count = 10, sum_loss = 573.9044189453125\n","batch_count = 10, sum_loss = 572.39013671875\n","batch_count = 10, sum_loss = 572.8110961914062\n","batch_count = 10, sum_loss = 571.451904296875\n","batch_count = 10, sum_loss = 569.6417236328125\n","batch_count = 10, sum_loss = 569.4827880859375\n","batch_count = 10, sum_loss = 574.3935546875\n","batch_count = 10, sum_loss = 571.8326416015625\n","batch_count = 10, sum_loss = 569.525146484375\n","batch_count = 10, sum_loss = 573.4774780273438\n","batch_count = 10, sum_loss = 573.1411743164062\n","batch_count = 10, sum_loss = 570.9224853515625\n","batch_count = 10, sum_loss = 568.76953125\n","batch_count = 10, sum_loss = 565.6868286132812\n","batch_count = 10, sum_loss = 568.423583984375\n","batch_count = 10, sum_loss = 566.0135498046875\n","batch_count = 10, sum_loss = 571.0783081054688\n","batch_count = 10, sum_loss = 562.987060546875\n","batch_count = 10, sum_loss = 565.20263671875\n","batch_count = 10, sum_loss = 566.175537109375\n","batch_count = 10, sum_loss = 559.8740844726562\n","batch_count = 10, sum_loss = 569.10546875\n","batch_count = 10, sum_loss = 563.49462890625\n","batch_count = 10, sum_loss = 567.2460327148438\n","batch_count = 10, sum_loss = 566.8466796875\n","batch_count = 10, sum_loss = 563.2401123046875\n","batch_count = 10, sum_loss = 562.9443359375\n","batch_count = 10, sum_loss = 563.213134765625\n","batch_count = 10, sum_loss = 563.4246826171875\n","batch_count = 10, sum_loss = 566.1362915039062\n","Storing the model after each epoch to compare the performance of them\n","EPOCH: 1 started\n","batch_count = 10, sum_loss = 563.6925659179688\n","batch_count = 10, sum_loss = 564.2974853515625\n","batch_count = 10, sum_loss = 560.1209716796875\n","batch_count = 10, sum_loss = 559.7984619140625\n","batch_count = 10, sum_loss = 558.3395385742188\n","batch_count = 10, sum_loss = 563.2064819335938\n","batch_count = 10, sum_loss = 557.0541381835938\n","batch_count = 10, sum_loss = 557.5739135742188\n","batch_count = 10, sum_loss = 560.4793090820312\n","batch_count = 10, sum_loss = 556.9359741210938\n","batch_count = 10, sum_loss = 560.6461791992188\n","batch_count = 10, sum_loss = 557.4974365234375\n","batch_count = 10, sum_loss = 558.4281616210938\n","batch_count = 10, sum_loss = 552.4721069335938\n","batch_count = 10, sum_loss = 559.318115234375\n","batch_count = 10, sum_loss = 558.330078125\n","batch_count = 10, sum_loss = 559.3828735351562\n","batch_count = 10, sum_loss = 556.16162109375\n","batch_count = 10, sum_loss = 554.3282470703125\n","batch_count = 10, sum_loss = 555.241455078125\n","batch_count = 10, sum_loss = 555.3991088867188\n","batch_count = 10, sum_loss = 555.862548828125\n","batch_count = 10, sum_loss = 556.1015625\n","batch_count = 10, sum_loss = 557.06396484375\n","batch_count = 10, sum_loss = 555.094482421875\n","batch_count = 10, sum_loss = 551.8031005859375\n","batch_count = 10, sum_loss = 554.6575927734375\n","batch_count = 10, sum_loss = 553.5999755859375\n","batch_count = 10, sum_loss = 556.4539184570312\n","batch_count = 10, sum_loss = 555.2214965820312\n","batch_count = 10, sum_loss = 557.3551025390625\n","batch_count = 10, sum_loss = 554.0421752929688\n","batch_count = 10, sum_loss = 550.7725830078125\n","batch_count = 10, sum_loss = 556.4121704101562\n","batch_count = 10, sum_loss = 551.3461303710938\n","batch_count = 10, sum_loss = 551.2764282226562\n","batch_count = 10, sum_loss = 551.23681640625\n","batch_count = 10, sum_loss = 552.5668334960938\n","batch_count = 10, sum_loss = 550.3433227539062\n","batch_count = 10, sum_loss = 553.8800048828125\n","batch_count = 10, sum_loss = 548.6171264648438\n","batch_count = 10, sum_loss = 549.712158203125\n","batch_count = 10, sum_loss = 547.9431762695312\n","batch_count = 10, sum_loss = 550.0929565429688\n","batch_count = 10, sum_loss = 552.1414184570312\n","batch_count = 10, sum_loss = 547.0140380859375\n","batch_count = 10, sum_loss = 550.9122314453125\n","batch_count = 10, sum_loss = 547.8986206054688\n","batch_count = 10, sum_loss = 552.7295532226562\n","batch_count = 10, sum_loss = 549.52587890625\n","batch_count = 10, sum_loss = 550.3336181640625\n","batch_count = 10, sum_loss = 550.14990234375\n","batch_count = 10, sum_loss = 552.6346435546875\n","batch_count = 10, sum_loss = 548.1115112304688\n","batch_count = 10, sum_loss = 553.8740844726562\n","batch_count = 10, sum_loss = 547.7728271484375\n","batch_count = 10, sum_loss = 547.6317138671875\n","batch_count = 10, sum_loss = 544.2713012695312\n","batch_count = 10, sum_loss = 547.373779296875\n","batch_count = 10, sum_loss = 546.506103515625\n","batch_count = 10, sum_loss = 545.9244995117188\n","batch_count = 10, sum_loss = 545.9168701171875\n","batch_count = 10, sum_loss = 546.441162109375\n","batch_count = 10, sum_loss = 548.4759521484375\n","batch_count = 10, sum_loss = 543.9461669921875\n","batch_count = 10, sum_loss = 551.02783203125\n","batch_count = 10, sum_loss = 544.61572265625\n"]}],"source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","\n","BATCH_SIZE = 16\n","EPOCHS = 3\n","LEARNING_RATE = 3e-5\n","WARMUP_STEPS = 5000\n","MAX_SEQ_LEN = 400\n","\n","# Train the model and save the model weights after each epoch and then generate jokes with each version of the weight \n","# to see which performs the best.\n","\n","model = model.to(device)\n","model.train()\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\n","\n","proc_seq_count = 0\n","sum_loss = 0.0\n","batch_count = 0\n","tmp_jokes_tens = None\n","\n","models_folder = jokes_dataset_path + \"trained_models\"\n","if not os.path.exists(models_folder):\n","    os.mkdir(models_folder)\n","\n","for epoch in range(EPOCHS):\n","    print(f\"EPOCH: {epoch} started\")\n","    for idx, joke in enumerate(joke_loader):\n","        # print(f\"Starting with idx: {idx}, joke: {joke}\")\n","        \n","        # Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\n","        joke_tens = torch.tensor(tokenizer.encode(joke[0])).unsqueeze(0).to(device)\n","        \n","        # Skip sample from dataset if it is longer than MAX_SEQ_LEN\n","        if joke_tens.size()[1] \u003e MAX_SEQ_LEN:\n","            continue\n","        \n","        # The first joke sequence in the sequence\n","        if not torch.is_tensor(tmp_jokes_tens):\n","            tmp_jokes_tens = joke_tens\n","            continue\n","        else:\n","            # The next joke does not fit in so we process the sequence and leave the last joke as the start for next sequence \n","            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] \u003e MAX_SEQ_LEN:\n","                work_jokes_tens = tmp_jokes_tens\n","                tmp_jokes_tens = joke_tens\n","            else:\n","                # Add the joke to sequence, continue and try to add more\n","                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n","                continue\n","\n","        # Sequence ready, process it trough the model\n","        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n","        loss, logits = outputs[:2]                        \n","        loss.backward()\n","        sum_loss = sum_loss + loss.detach().data\n","                       \n","        proc_seq_count = proc_seq_count + 1\n","        if proc_seq_count == BATCH_SIZE:\n","            proc_seq_count = 0    \n","            batch_count += 1\n","            optimizer.step()\n","            scheduler.step() \n","            optimizer.zero_grad()\n","            model.zero_grad()\n","\n","        if batch_count == 10:\n","            print(f\"batch_count = {batch_count}, sum_loss = {sum_loss}\")\n","            batch_count, sum_loss = 0, 0.0\n","    \n","    print(\"Storing the model after each epoch to compare the performance of them\")\n","    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_small_joker_{epoch}.pt\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PtAgnXUfYuND"},"outputs":[],"source":["\"\"\"\n","Generating the jokes\n","\"\"\"\n","MODEL_EPOCH = 3\n","model_path = os.path.join(models_folder, f\"gpt2_small_joker_{MODEL_EPOCH}.pt\")\n","model.load_state_dict(torch.load(model_path))\n","\n","jokes_output_file_path = jokes_dataset_path + f\"generated_{MODEL_EPOCH}.jokes\"\n","\n","model.eval()\n","if os.path.exists(jokes_output_file_path):\n","    os.remove(jokes_output_file_path)\n","    \n","joke_num = 0\n","with torch.no_grad():\n","        for joke_idx in range(1000):\n","        \n","            joke_finished = False\n","            cur_ids = torch.tensor(tokenizer.encode(\"JOKE:\")).unsqueeze(0).to(device)\n","\n","            for i in range(100):\n","                outputs = model(cur_ids, labels=cur_ids)\n","                loss, logits = outputs[:2]\n","                softmax_logits = torch.softmax(logits[0,-1], dim=0) # Take the first(from only one in this case) batch and the last predicted embedding\n","                if i \u003c 3:\n","                    n = 20\n","                else:\n","                    n = 3\n","                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n","                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n","\n","                if next_token_id in tokenizer.encode('\u003c|endoftext|\u003e'):\n","                    joke_finished = True\n","                    break\n","\n","            \n","            if joke_finished:\n","                joke_num = joke_num + 1\n","                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n","                output_text = tokenizer.decode(output_list)\n","                with open(jokes_output_file_path, 'a') as f:\n","                    f.write(f\"{output_text} \\n\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZHDiMhOh9Dr"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"gpt2-fine-tune.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"058acabe1f7547f5a3fbbd7acac66d80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06f67a089a72419a94e3d807e2ae87a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0bed888db97e4bd799afa17a95095bdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18fd8a4843ad4f48afc3cb9565d6352e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_22c9ddae6faa430dbe3eaf81d3127ae6","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a9ff88b2b1164f24acfece8f8e2c8d0f","value":456318}},"1ada09cb11c74d04901f02f17dcab2ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_7d9795cabdfc40548bb2289a54fabe02","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dbe15ddcd1644631b06f65948a429b3c","value":665}},"22c9ddae6faa430dbe3eaf81d3127ae6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22d774b186c940d3b9108efe3ecd9cd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"233eedd48c324c9da6fcc58896f024f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"2f1958ed6b40476f96dcb6316d7d24af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c93e5710fbf43268e8f88c30b5c437c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45849addc98f4b85918206c7f249599f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bd42ac7e92a4b1f92849e47c02c6256":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_bc7e787671864583bf5a73ae2d1eec1c","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c92c9a0e95b4b6ebb09fe88f3525e65","value":1042301}},"4d1a43d9541740d5a6641e1d1bc68a9a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_527246e824aa4844970e4fafab3adfab","IPY_MODEL_61ca474a6fb1402b85f2037349e75a47"],"layout":"IPY_MODEL_ea926d03fee144658be81dbce4ade176"}},"4d9631485f33475d8c6758c08bc8c770":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4de7d6b7707d4b8692970d2018f55012","placeholder":"​","style":"IPY_MODEL_06f67a089a72419a94e3d807e2ae87a1","value":" 1.04M/1.04M [00:01\u0026lt;00:00, 606kB/s]"}},"4de7d6b7707d4b8692970d2018f55012":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"527246e824aa4844970e4fafab3adfab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_45849addc98f4b85918206c7f249599f","max":548118077,"min":0,"orientation":"horizontal","style":"IPY_MODEL_233eedd48c324c9da6fcc58896f024f8","value":548118077}},"61ca474a6fb1402b85f2037349e75a47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeb5bf0d0bdb451fb1a247288ab06f0a","placeholder":"​","style":"IPY_MODEL_88d77e59fde14fee88f6e451314642fd","value":" 548M/548M [00:24\u0026lt;00:00, 22.6MB/s]"}},"6fb0f54c099f43c68c80618bb37ccfff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d9795cabdfc40548bb2289a54fabe02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88d77e59fde14fee88f6e451314642fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c92c9a0e95b4b6ebb09fe88f3525e65":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"a9ff88b2b1164f24acfece8f8e2c8d0f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"b80ad7b214a64d50a68bd3a3be5f293c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc7e787671864583bf5a73ae2d1eec1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf9056f6b35441b68bc983441937c9d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18fd8a4843ad4f48afc3cb9565d6352e","IPY_MODEL_f7641fb71080497c8e3096b37388e29f"],"layout":"IPY_MODEL_058acabe1f7547f5a3fbbd7acac66d80"}},"c2a94126efe7456ca541b877625c788d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4bd42ac7e92a4b1f92849e47c02c6256","IPY_MODEL_4d9631485f33475d8c6758c08bc8c770"],"layout":"IPY_MODEL_6fb0f54c099f43c68c80618bb37ccfff"}},"d91a6b0f40064a0ab6944f5a5bf71c18":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ada09cb11c74d04901f02f17dcab2ef","IPY_MODEL_dd94af6b6e6e4196bf9012a1fbccc9c0"],"layout":"IPY_MODEL_22d774b186c940d3b9108efe3ecd9cd1"}},"dbe15ddcd1644631b06f65948a429b3c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"dd94af6b6e6e4196bf9012a1fbccc9c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b80ad7b214a64d50a68bd3a3be5f293c","placeholder":"​","style":"IPY_MODEL_3c93e5710fbf43268e8f88c30b5c437c","value":" 665/665 [00:59\u0026lt;00:00, 11.2B/s]"}},"ea926d03fee144658be81dbce4ade176":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeb5bf0d0bdb451fb1a247288ab06f0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7641fb71080497c8e3096b37388e29f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0bed888db97e4bd799afa17a95095bdc","placeholder":"​","style":"IPY_MODEL_2f1958ed6b40476f96dcb6316d7d24af","value":" 456k/456k [00:00\u0026lt;00:00, 1.04MB/s]"}}}}},"nbformat":4,"nbformat_minor":0}