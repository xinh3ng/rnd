{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "gpt2-fine-tune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinh3ng/ds-research/blob/colab/gpt2_fine_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JWjZskRT1mr"
      },
      "source": [
        "**Links:**\n",
        "\n",
        "*   https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912\n",
        "*   https://gist.github.com/mf1024/3df214d2f17f3dcc56450ddf0d5a4cd7#file-fine-tuning-gpt2-medium-in-pytorch-ipynb\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih_TLLX0T1ms"
      },
      "source": [
        "# Generating text with a pre-trained GPT2 in PyTorch\n",
        "\n",
        "This notebook was created as a part of a blog post - [Fine-tuning large Transformer models on a single GPU in PyTorch - Teaching GPT-2 a sense of humor](https://mf1024.github.io/2019/11/12/Fun-With-GPT-2/).\n",
        "\n",
        "In this notebook, I will use a pre-trained medium-sized GPT2 model from the [huggingface](https://github.com/huggingface/transformers) to generate some text.\n",
        "\n",
        "The easiest way to use huggingface transformer libraries is to install their pip package *transformers*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVi53PP4T1mt",
        "outputId": "ba73cafc-335b-47c7-80e0-230e34fea3f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KrlR4r4T1mw",
        "outputId": "9d5944ab-ee73-47b1-ec6e-3fc0515d7be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "logging.getLogger().setLevel(logging.CRITICAL)\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "\n",
        "pd.set_option(\"precision\", 4)\n",
        "\n",
        "print(\"Python version is %s\" % sys.version)\n",
        "print(\"Device is: %s\" % device)\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version is 3.6.9 (default, Jul 17 2020, 12:50:27) \n",
            "[GCC 8.4.0]\n",
            "Device is: cuda\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EScszWmxT1mz"
      },
      "source": [
        "### Models and classes\n",
        "\n",
        "I use the [GPT2LMHeadModel](https://github.com/huggingface/transformers/blob/master/transformers/modeling_gpt2.py#L491) module for the language model, which is [GPT2Model](https://github.com/huggingface/transformers/blob/master/transformers/modeling_gpt2.py#L326), with an additional linear layer that uses input embedding layer weights to do the inverse operation of the embedding layer - to create logits vector for the dictionary from outputs of the GPT2.\n",
        "\n",
        "[GPT2Tokenizer](https://github.com/huggingface/transformers/blob/master/transformers/tokenization_gpt2.py#L106) is a byte-code pair encoder that will transform input text input into input tokens that the huggingface transformers were trained on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUX7jIfoT1m0",
        "outputId": "42d68379-ea54-4fbd-baf9-68b03ebd6e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model = model.to(device)\n",
        "print(\"model has %s Bytes\" % sys.getsizeof(model))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model has 56 Bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUkseDkIT1m3"
      },
      "source": [
        "def choose_from_top(probs: list, n: int = 5):\n",
        "    \"\"\"Select topN tokens from the probability list. Then based on the selected N word distribution get random token ID\n",
        "    \"\"\"\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWCSFbfmT1m5"
      },
      "source": [
        "### Text generation\n",
        "\n",
        "At each prediction step, GPT2 model needs to know all of the previous sequence elements to predict the next one. Below is a function that will tokenize the starting input text, and then in a loop, one new token is predicted at each step and is added to the sequence, which will be fed into the model in the next step. In the end, the token list is decoded back into a text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiZtB09ET1m5"
      },
      "source": [
        "def generate_some_text(input_str, text_len = 250):\n",
        "    cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(text_len):\n",
        "            outputs = model(cur_ids, labels=cur_ids)\n",
        "            loss, logits = outputs[:2]\n",
        "\n",
        "            # Take the first(only one) batch and the last predicted embedding\n",
        "            softmax_logits = torch.softmax(logits[0,-1], dim=0) \n",
        "            \n",
        "            # Randomly(from the given probability distribution) choose the next word from the top n words\n",
        "            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=10) \n",
        "            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word\n",
        "\n",
        "        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "        output_text = tokenizer.decode(output_list)\n",
        "        print(output_text)\n",
        "    \n",
        "    return\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxZE8sBUT1m8"
      },
      "source": [
        "## Generating the text\n",
        "\n",
        "I will give thre different sentence beginnings to the GPT2 and let it generate the rest:\n",
        "\n",
        "\n",
        "***1. The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work… when you go to church… when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth…***\n",
        "\n",
        "***2. Artificial general intelligence is…***\n",
        "\n",
        "***3. The Godfather: “I’m going to make him an offer he can’t refuse.”…***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec1tIq_tT1m8",
        "outputId": "3980fc9b-1422-4a61-d3d4-f1d9aa561a9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "generate_some_text(\"The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work... when you go to church... when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth. \")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work... when you go to church... when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth.  You must learn to understand it.\"\n",
            " \"I'm not saying you should be scared to die. But I'm saying that you should learn to understand it.\"   The Matrix is all around us. In our heads, we see everything. The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work... when you go to church... when your wages are paid. It is the world that has been pulled over your eyes to blind you from the truth.  You must learn to understand it.\" \"I'm not saying you should be scared to die. But I'm saying that you should learn to understand it.\"\n",
            "\"It is the world that has been pulled over your eyes to blind you from the truth.  You must learn to understand it.\" \"I'm not saying you should be scared to die.  But I'm saying that you should learn to understand it.\" \"I'm not saying you should be afraid to die.  But I'm saying that you should learn to understand it.\"\n",
            "I have been reading about this Matrix for\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLphHAyNT1nA",
        "outputId": "b008af12-ba26-4b58-a3a8-c7df2a188c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "generate_some_text(\" Artificial general intelligence is \")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Artificial general intelligence is vernacular as it is. It's a great tool for the development of new ideas that might otherwise only be explored by people who know nothing about science or computer science. In short, it's great for developing new ideas that will help us better understand the world around us.\n",
            "\n",
            "There is a great deal of work that needs to be done to make AI the most useful tool we can be. There is a lot more that needs to be done, but at the moment there are two main things we do:\n",
            "\n",
            "We need to be able to use artificial intelligence to solve problems and solve problems that don't exist,\n",
            "\n",
            "We need to be able to use artificial intelligence to help solve problems that are not actually human in nature, and\n",
            "\n",
            "We need to be able to use artificial intelligence to solve problems that do not exist and do not have any human involvement whatsoever.\n",
            "\n",
            "So let me give an example. Let's say we want to solve a problem. We have two main goals. One is to understand the physics of the problem. The other is to understand the physics of how the problem is solved.\n",
            "\n",
            "Let's look at two problems we have. We want to learn about the physics of the problem and we want to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1i7X_39T1nD",
        "outputId": "92bf41d4-8ef0-47c7-cd43-16136f7849a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "generate_some_text(\" The Godfather: \\\"I'm going to make him an offer he can't refuse.\\\" \")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The Godfather: \"I'm going to make him an offer he can't refuse.\"  And it was just such a deal.  He offered to pay his $5,000 to get him to take a photo of himself on his Facebook page and post it.  He also offered a $10,000 reward for those who could get him to write down the name of his mother and get her to take the picture.  So I was pretty excited at that point.  I think the reason why I was so excited was because I thought there could be a way around it because I think the way people were going to react when I posted was that they would say \"you know what, you're really a man!\" It's like I thought \"I'm just being nice and I'm not being mean!\" But that's not how the world works.  It's not like the world works for you. I've had people say, \"You know, I want to be a guy and that's why I'm doing that.\" But I think it was just like it was the only answer that I'd ever get from people that said, \"Well I want your money for that.\"  And it's just like you've got to figure things out.  You never say never. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBmcRsiIVgGy"
      },
      "source": [
        "\"\"\"\n",
        "Jokes data set\n",
        "\"\"\"\n",
        "import csv\n",
        "import os\n",
        "import json\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class JokesDataset(Dataset):\n",
        "    def __init__(self, jokes_dataset_path: str):\n",
        "        super().__init__()\n",
        "        short_jokes_path = os.path.join(jokes_dataset_path, \"shortjokes.csv\")\n",
        "        self.joke_list = []\n",
        "        self.end_of_text_token = \"<|endoftext|>\"\n",
        "        \n",
        "        with open(short_jokes_path) as csv_file:\n",
        "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "            x = 0\n",
        "            for row in csv_reader:\n",
        "                joke_str = f\"JOKE:{row[1]}{self.end_of_text_token}\"\n",
        "                self.joke_list.append(joke_str)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.joke_list)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.joke_list[item]\n",
        "\n",
        "jokes_dataset_path = \"/content/gdrive/My Drive/xheng/data/jokes_data/\"  # flower dataset's path\n",
        "\n",
        "dataset = JokesDataset(jokes_dataset_path=jokes_dataset_path)\n",
        "joke_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_aFvphHV94i",
        "outputId": "ef1e6743-274b-4ae1-d1ce-6eb332a8126f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 3e-5\n",
        "WARMUP_STEPS = 5000\n",
        "MAX_SEQ_LEN = 400\n",
        "\n",
        "# Train the model and save the model weights after each epoch and then generate jokes with each version of the weight \n",
        "# to see which performs the best.\n",
        "\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\n",
        "\n",
        "proc_seq_count = 0\n",
        "sum_loss = 0.0\n",
        "batch_count = 0\n",
        "tmp_jokes_tens = None\n",
        "\n",
        "models_folder = jokes_dataset_path + \"trained_models\"\n",
        "if not os.path.exists(models_folder):\n",
        "    os.mkdir(models_folder)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"EPOCH: {epoch} started\")\n",
        "    for idx, joke in enumerate(joke_loader):\n",
        "        # print(f\"Starting with idx: {idx}, joke: {joke}\")\n",
        "        \n",
        "        # Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\n",
        "        joke_tens = torch.tensor(tokenizer.encode(joke[0])).unsqueeze(0).to(device)\n",
        "        \n",
        "        # Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
        "        if joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "            continue\n",
        "        \n",
        "        # The first joke sequence in the sequence\n",
        "        if not torch.is_tensor(tmp_jokes_tens):\n",
        "            tmp_jokes_tens = joke_tens\n",
        "            continue\n",
        "        else:\n",
        "            # The next joke does not fit in so we process the sequence and leave the last joke as the start for next sequence \n",
        "            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "                work_jokes_tens = tmp_jokes_tens\n",
        "                tmp_jokes_tens = joke_tens\n",
        "            else:\n",
        "                # Add the joke to sequence, continue and try to add more\n",
        "                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n",
        "                continue\n",
        "\n",
        "        # Sequence ready, process it trough the model\n",
        "        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n",
        "        loss, logits = outputs[:2]                        \n",
        "        loss.backward()\n",
        "        sum_loss = sum_loss + loss.detach().data\n",
        "                       \n",
        "        proc_seq_count = proc_seq_count + 1\n",
        "        if proc_seq_count == BATCH_SIZE:\n",
        "            proc_seq_count = 0    \n",
        "            batch_count += 1\n",
        "            optimizer.step()\n",
        "            scheduler.step() \n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "        if batch_count == 10:\n",
        "            print(f\"batch_count = {batch_count}, sum_loss = {sum_loss}\")\n",
        "            batch_count, sum_loss = 0, 0.0\n",
        "    \n",
        "    print(\"Storing the model after each epoch to compare the performance of them\")\n",
        "    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_small_joker_{epoch}.pt\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0 started\n",
            "batch_count = 10, sum_loss = 726.8229370117188\n",
            "batch_count = 10, sum_loss = 728.3819580078125\n",
            "batch_count = 10, sum_loss = 719.9417724609375\n",
            "batch_count = 10, sum_loss = 720.7799682617188\n",
            "batch_count = 10, sum_loss = 721.210693359375\n",
            "batch_count = 10, sum_loss = 717.4074096679688\n",
            "batch_count = 10, sum_loss = 710.0521240234375\n",
            "batch_count = 10, sum_loss = 711.2081298828125\n",
            "batch_count = 10, sum_loss = 705.300537109375\n",
            "batch_count = 10, sum_loss = 698.4765014648438\n",
            "batch_count = 10, sum_loss = 700.3212890625\n",
            "batch_count = 10, sum_loss = 691.4007568359375\n",
            "batch_count = 10, sum_loss = 684.748291015625\n",
            "batch_count = 10, sum_loss = 682.7960205078125\n",
            "batch_count = 10, sum_loss = 680.3382568359375\n",
            "batch_count = 10, sum_loss = 670.200927734375\n",
            "batch_count = 10, sum_loss = 670.744140625\n",
            "batch_count = 10, sum_loss = 662.0491943359375\n",
            "batch_count = 10, sum_loss = 650.756103515625\n",
            "batch_count = 10, sum_loss = 642.2711791992188\n",
            "batch_count = 10, sum_loss = 630.345947265625\n",
            "batch_count = 10, sum_loss = 629.2499389648438\n",
            "batch_count = 10, sum_loss = 617.5452270507812\n",
            "batch_count = 10, sum_loss = 611.0418701171875\n",
            "batch_count = 10, sum_loss = 612.54931640625\n",
            "batch_count = 10, sum_loss = 615.9014892578125\n",
            "batch_count = 10, sum_loss = 605.09033203125\n",
            "batch_count = 10, sum_loss = 610.7592163085938\n",
            "batch_count = 10, sum_loss = 606.2019653320312\n",
            "batch_count = 10, sum_loss = 606.0150146484375\n",
            "batch_count = 10, sum_loss = 603.4300537109375\n",
            "batch_count = 10, sum_loss = 606.4420166015625\n",
            "batch_count = 10, sum_loss = 598.5345458984375\n",
            "batch_count = 10, sum_loss = 601.5394897460938\n",
            "batch_count = 10, sum_loss = 597.2028198242188\n",
            "batch_count = 10, sum_loss = 602.496337890625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtAgnXUfYuND"
      },
      "source": [
        "\"\"\"\n",
        "Generating the jokes\n",
        "\"\"\"\n",
        "MODEL_EPOCH = 4\n",
        "model_path = os.path.join(models_folder, f\"gpt2_small_joker_{MODEL_EPOCH}.pt\")\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "jokes_output_file_path = jokes_dataset_path + f\"generated_{MODEL_EPOCH}.jokes\"\n",
        "\n",
        "model.eval()\n",
        "if os.path.exists(jokes_output_file_path):\n",
        "    os.remove(jokes_output_file_path)\n",
        "    \n",
        "joke_num = 0\n",
        "with torch.no_grad():\n",
        "        for joke_idx in range(1000):\n",
        "        \n",
        "            joke_finished = False\n",
        "            cur_ids = torch.tensor(tokenizer.encode(\"JOKE:\")).unsqueeze(0).to(device)\n",
        "\n",
        "            for i in range(100):\n",
        "                outputs = model(cur_ids, labels=cur_ids)\n",
        "                loss, logits = outputs[:2]\n",
        "                softmax_logits = torch.softmax(logits[0,-1], dim=0) # Take the first(from only one in this case) batch and the last predicted embedding\n",
        "                if i < 3:\n",
        "                    n = 20\n",
        "                else:\n",
        "                    n = 3\n",
        "                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n",
        "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
        "\n",
        "                if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
        "                    joke_finished = True\n",
        "                    break\n",
        "\n",
        "            \n",
        "            if joke_finished:\n",
        "                joke_num = joke_num + 1\n",
        "                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "                output_text = tokenizer.decode(output_list)\n",
        "                with open(jokes_output_file_path, 'a') as f:\n",
        "                    f.write(f\"{output_text} \\n\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZHDiMhOh9Dr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}