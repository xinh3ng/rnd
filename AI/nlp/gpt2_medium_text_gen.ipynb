{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "colab": {
   "name": "gpt2-medium-text-gen.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/xinh3ng/ds-research/blob/colab/gpt2_medium_text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JWjZskRT1mr"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ih_TLLX0T1ms"
   },
   "source": [
    "# Generating text with a pre-trained GPT2 in PyTorch\n",
    "\n",
    "This notebook was created as a part of a blog post - [Fine-tuning large Transformer models on a single GPU in PyTorch - Teaching GPT-2 a sense of humor](https://mf1024.github.io/2019/11/12/Fun-With-GPT-2/).\n",
    "\n",
    "In this notebook, I will use a pre-trained medium-sized GPT2 model from the [huggingface](https://github.com/huggingface/transformers) to generate some text.\n",
    "\n",
    "The easiest way to use huggingface transformer libraries is to install their pip package *transformers*."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JVi53PP4T1mt",
    "outputId": "8ad0f04f-3aec-4f82-fffd-41659a45c311",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    }
   },
   "source": [
    "!pip install transformers"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 4.6MB/s \n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 25.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 40.3MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 53.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=ca2e070a85360d92ddb2d4fcd95ae42fef37f6d33e8d0d044db834c3b8bfc615\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7KrlR4r4T1mw",
    "outputId": "576c3e31-3bc3-4659-bdaa-32dfcca8fa70",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    }
   },
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "print(\"Python version is %s\" % sys.version)\n",
    "print(\"Device is: %s\" % device)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Python version is 3.6.9 (default, Jul 17 2020, 12:50:27) \n",
      "[GCC 8.4.0]\n",
      "Device is: cuda\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EScszWmxT1mz"
   },
   "source": [
    "### Models and classes\n",
    "\n",
    "I use the [GPT2LMHeadModel](https://github.com/huggingface/transformers/blob/master/transformers/modeling_gpt2.py#L491) module for the language model, which is [GPT2Model](https://github.com/huggingface/transformers/blob/master/transformers/modeling_gpt2.py#L326), with an additional linear layer that uses input embedding layer weights to do the inverse operation of the embedding layer - to create logits vector for the dictionary from outputs of the GPT2.\n",
    "\n",
    "[GPT2Tokenizer](https://github.com/huggingface/transformers/blob/master/transformers/tokenization_gpt2.py#L106) is a byte-code pair encoder that will transform input text input into input tokens that the huggingface transformers were trained on. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zUX7jIfoT1m0",
    "outputId": "1f7d2a31-3103-4994-fcec-a5928c4fdcf9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    }
   },
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"model has %s B\" % sys.getsizeof(model))"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "model has 56 B\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uUkseDkIT1m3"
   },
   "source": [
    "def choose_from_top(probs: list, n: int = 5):\n",
    "    \"\"\"Select topN tokens from the probability list. Then based on the selected N word distribution get random token ID\"\"\"\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob)  # Normalize\n",
    "    choice = np.random.choice(n, 1, p=top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWCSFbfmT1m5"
   },
   "source": [
    "### Text generation\n",
    "\n",
    "At each prediction step, GPT2 model needs to know all of the previous sequence elements to predict the next one. Below is a function that will tokenize the starting input text, and then in a loop, one new token is predicted at each step and is added to the sequence, which will be fed into the model in the next step. In the end, the token list is decoded back into a text. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KiZtB09ET1m5"
   },
   "source": [
    "def generate_some_text(input_str, text_len=250):\n",
    "    cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(text_len):\n",
    "            outputs = model(cur_ids, labels=cur_ids)\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            # Take the first(only one) batch and the last predicted embedding\n",
    "            softmax_logits = torch.softmax(logits[0, -1], dim=0)\n",
    "\n",
    "            # Randomly(from the given probability distribution) choose the next word from the top n words\n",
    "            next_token_id = choose_from_top(softmax_logits.to(\"cpu\").numpy(), n=10)\n",
    "            cur_ids = torch.cat(\n",
    "                [cur_ids, torch.ones((1, 1)).long().to(device) * next_token_id], dim=1\n",
    "            )  # Add the last word\n",
    "\n",
    "        output_list = list(cur_ids.squeeze().to(\"cpu\").numpy())\n",
    "        output_text = tokenizer.decode(output_list)\n",
    "        print(output_text)\n",
    "\n",
    "    return"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxZE8sBUT1m8"
   },
   "source": [
    "## Generating the text\n",
    "\n",
    "I will give thre different sentence beginnings to the GPT2 and let it generate the rest:\n",
    "\n",
    "\n",
    "***1. The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work… when you go to church… when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth…***\n",
    "\n",
    "***2. Artificial general intelligence is…***\n",
    "\n",
    "***3. The Godfather: “I’m going to make him an offer he can’t refuse.”…***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ec1tIq_tT1m8",
    "outputId": "6f29f5ce-6e3a-46ed-adf1-07d16888af48",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    }
   },
   "source": [
    "generate_some_text(\n",
    "    \"The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work... when you go to church... when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth. \"\n",
    ")"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work... when you go to church... when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth.  The truth is that you are the Matrix. The truth about the world. The whole truth. The Matrix is not a movie. It is a reality.  And the truth is that you are in the Matrix, not in this room.\n",
      "The Matrix is not a movie. The truth is that you are the Matrix. The truth is that you are in the Matrix\n",
      "The Matrix is not a movie. It is the world that has been pulled over your eyes to blind you from the truth\n",
      "And the truth is that you are in the Matrix, not in this room.\n",
      "The Matrix is not a movie. It is the World that has been pulled over your eyes to blind you from the truth\n",
      "And the Truth, the Matrix, is not a movie. The truth is that you are the Matrix. The truth is that you are in the Matrix, not in this room.\n",
      "The Matrix is not a movie. The truth is that you are the Matrix. The truth is that you are in the Matrix, not in this room.\n",
      "The Matrix is not a movie. It is the world that has been pulled over your eyes to blind you from the truth\n",
      "The truth is that you are the Matrix. The truth is\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XLphHAyNT1nA",
    "outputId": "c15b9051-ffe1-4034-caf8-629afc5c1ee0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    }
   },
   "source": [
    "generate_some_text(\" Artificial general intelligence is \")"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      " Artificial general intelligence is  a concept that is based on a lot of scientific and mathematical concepts.  The definition  of an intelligent being (i.e. computer) is based on what is known as Turing Machines (TM).  The most famous example of a Turing machine is Alan Turing, who was a computer scientist and mathematician, and who built the Turing machine that is known as Alan Turing (or  Alan).\n",
      "Turing Machine (turing machine.jpg) Alan Turing was a computer scientist and mathematician.  He built the Turing machine that is known as Alan Turing (or  Alan).  The definition of an intelligent being  is based on  turing machines (TM).  It is important to note that  turing machines are machines that have been programmed with certain goals, such as the definition  of intelligence (see the Wikipedia article here.)  The goal of a turing machine (TM) is  to find a way to improve  on the capabilities that it has learned.  A turing machine has a very generalizable programming language (or \n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z1i7X_39T1nD",
    "outputId": "910d8717-3b4d-46c4-fc10-681dd236030a"
   },
   "source": [
    "generate_some_text(\" The Godfather: \\\"I'm going to make him an offer he can't refuse.\\\" \")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "The Godfather: \"I'm going to make him an offer he can't refuse.\"\n",
      "\n",
      "The Godfather: \"What? What is it? He has to be a good boy? A good boy that doesn't want to be killed? Is the offer good?\"\n",
      "\n",
      "The Godfather: \"He's a bad boy, isn't he.\"\n",
      "\n",
      "The Godfather: \"You're a good boy!\"\n",
      "\n",
      "The Godfather: \"He's an idiot. He won't be able to understand what's going on!\"\n",
      "\n",
      "The Godfather: \"You know, I never said you would be able to understand what's going on! I said you would be able to take him to a friend's house.\"\n",
      "\n",
      "The Godfather: \"I don't understand! You mean you'll never understand what's going on? What's happening to me?\"\n",
      "\n",
      "The Godfather: \"That's the only way I can explain it to him. He's not going to be able to understand it either if I tell him what I know. He won't be able even to comprehend a thing if I tell him what it is.\"\n",
      "\n",
      "The Godfather: \"Well, you know, I've seen it all. I don't know what he will do. And, if he does, what's\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}